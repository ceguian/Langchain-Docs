{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d7fb7a-79d3-41ec-86c0-0fb45282f1fd",
   "metadata": {},
   "source": [
    "# Construyendo una aplicación simple de LLM\n",
    "\n",
    "En esta guía de inicio rápido, le mostraremos cómo crear una aplicación LLM sencilla con LangChain. Esta aplicación traducirá texto del inglés a otro idioma. Es una aplicación LLM relativamente sencilla: solo requiere una llamada LLM y algunas indicaciones. Aun así, es una excelente manera de comenzar con LangChain: ¡se pueden crear muchas funciones con solo algunas indicaciones y una llamada LLM!\n",
    "\n",
    "Después de leer este tutorial, tendrá una visión general de:\n",
    "\n",
    "- Uso de modelos de lenguaje\n",
    "\n",
    "- Uso de plantillas de indicaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778bb56-4fba-42c6-a0c2-36913fb1eb9c",
   "metadata": {},
   "source": [
    "## Bibliotecas\n",
    "- langchain\n",
    "- langchain_ollama\n",
    "- langchain_core\n",
    "```bash\n",
    "pip install langchain langchain_ollama langchain_core\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fbb3ce-b471-40a3-b197-d2a53c599869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987cafd-272a-48c4-b342-950a2cd47203",
   "metadata": {},
   "source": [
    "## Usando Modelos Largos de Lenguaje Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f14eb94-88f7-40cb-8300-935e4d80ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2\",                  # Modelo descargado desde Ollama (puedes cambiar el nombre según el modelo disponible)\n",
    "    base_url=\"http://localhost:11434\"  # URL local donde corre el servidor de Ollama\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af144f3a-1d9f-4688-86ca-2f28833476b5",
   "metadata": {},
   "source": [
    "## Usando el LLM\n",
    "Primero, usemos el modelo directamente. Los ChatModels son instancias de los Runnables de LangChain, lo que significa que exponen una interfaz estándar para interactuar con ellos. Para llamar al modelo, podemos pasar una lista de mensajes al método .invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d1b099-3f24-44b2-bc89-48664135876b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-09T23:43:51.63234Z', 'done': True, 'done_reason': 'stop', 'total_duration': 465263200, 'load_duration': 37064700, 'prompt_eval_count': 34, 'prompt_eval_duration': 103622700, 'eval_count': 4, 'eval_duration': 323202800, 'model_name': 'llama3.2'}, id='run-b6f94484-41b4-4cde-8883-17cce05d375b-0', usage_metadata={'input_tokens': 34, 'output_tokens': 4, 'total_tokens': 38})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using LLM\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "# Using dict of messages\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf177d-7fd1-4e84-ab63-ddb5366a8b3b",
   "metadata": {},
   "source": [
    "Tenga en cuenta que los ChatModels reciben objetos de mensaje como entrada y los generan como salida. Además del contenido textual, los objetos de mensaje transmiten roles conversacionales y contienen datos importantes, como llamadas a herramientas y recuentos de uso de tokens.\n",
    "\n",
    "LangChain también admite entradas de modelos de chat mediante cadenas o formato OpenAI. Los siguientes son equivalentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5698ea25-84c2-4761-ac50-82cf47e83967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-09T23:49:16.623069Z', 'done': True, 'done_reason': 'stop', 'total_duration': 796749600, 'load_duration': 28527600, 'prompt_eval_count': 26, 'prompt_eval_duration': 88446200, 'eval_count': 8, 'eval_duration': 679261700, 'model_name': 'llama3.2'}, id='run-58e56158-5cd7-41dd-888d-fe5c353c627c-0', usage_metadata={'input_tokens': 26, 'output_tokens': 8, 'total_tokens': 34})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using direct string\n",
    "model.invoke(\"Hello\")\n",
    "\n",
    "# Using dict with role and content\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n",
    "# Using Humanmessage with the prompt\n",
    "model.invoke([HumanMessage(\"Hello\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106e8b3-27aa-4e85-82fa-3f24c5144020",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "Dado que los modelos de chat son ejecutables, exponen una interfaz estándar que incluye modos de invocación asíncronos y de transmisión. Esto nos permite transmitir tokens individuales desde un modelo de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905270f7-1948-4a08-9706-c956f27b5fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C|iao|!||"
     ]
    }
   ],
   "source": [
    "#Streaming\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861abd6e-4e9a-4ded-9c51-72bd295994e8",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "Ahora mismo estamos pasando una lista de mensajes directamente al modelo de lenguaje. ¿De dónde proviene esta lista? Normalmente, se construye combinando la entrada del usuario y la lógica de la aplicación. Esta lógica de la aplicación suele tomar la entrada del usuario sin procesar y transformarla en una lista de mensajes listos para pasar al modelo de lenguaje. Las transformaciones comunes incluyen añadir un mensaje del sistema o formatear una plantilla con la entrada del usuario.\n",
    "\n",
    "Las plantillas de solicitud son un concepto de LangChain diseñado para facilitar esta transformación. Reciben la entrada del usuario sin procesar y devuelven datos (una solicitud) listos para pasar a un modelo de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce4ae4-621e-4391-9485-12a38f7b73e2",
   "metadata": {},
   "source": [
    "### String PromptTemplates\n",
    "Estas plantillas de solicitud se utilizan para dar formato a una sola cadena y, generalmente, se usan para entradas más sencillas.\n",
    "Por ejemplo, una forma común de construir y usar una plantilla de solicitud es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c39fb4-b7b3-49aa-b9b2-75d672f82a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Tell me a joke about cats'\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# Assing topic value with invoke\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b56aeb-804f-4cda-a113-4e31505a3f43",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates\n",
    "Estas plantillas de mensajes se utilizan para dar formato a una lista de mensajes.\n",
    "Estas \"plantillas\" consisten en una lista de plantillas.\n",
    "Por ejemplo, una forma común de crear y usar una ChatPromptTemplate es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b1fa1f-e003-4d38-8c70-17e81696d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bf515-2d54-4f02-8ed8-5c98d7899ab0",
   "metadata": {},
   "source": [
    "### MessagesPlaceholder\n",
    "Esta plantilla de mensaje se encarga de añadir una lista de mensajes en un lugar específico.\n",
    "En la plantilla ChatPromptTemplate anterior, vimos cómo formatear dos mensajes, cada uno como una cadena.\n",
    "¿Pero qué sucedería si quisiéramos que el usuario pasara una lista de mensajes que colocaríamos en un lugar específico?\n",
    "Así es como se usa MessagesPlaceholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a30c1d3-d662-42d0-afec-8beb7d6690da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5273ef4-81c0-4f05-879b-6d464373544e",
   "metadata": {},
   "source": [
    "Esto generará una lista de dos mensajes: el primero es un mensaje del sistema y el segundo es el HumanMessage que pasamos.\n",
    "Si hubiéramos pasado 5 mensajes, se habrían generado 6 mensajes en total (el mensaje del sistema más los 5 pasados).\n",
    "Esto es útil para permitir que una lista de mensajes se ubique en un lugar específico.\n",
    "\n",
    "Una forma alternativa de lograr lo mismo sin usar la clase MessagesPlaceholder explícitamente es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b12f59f-4aac-4354-8f43-56002a617997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716856b-a7a1-4e4c-87ae-0d66f842c8c8",
   "metadata": {},
   "source": [
    "Creemos una plantilla de solicitud aquí. Aceptará dos variables de usuario:\n",
    "\n",
    "- idioma: el idioma al que se traducirá el texto\n",
    "- texto: el texto a traducir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ec04234-1f8e-400d-bdd4-0ce0c11327dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "# Prompt template with dict of messages using from_messages method\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043741b6-3a8f-43b2-95fb-e67b1cf210f2",
   "metadata": {},
   "source": [
    "Tenga en cuenta que ChatPromptTemplate admite varios roles de mensaje en una sola plantilla. Formateamos el parámetro de idioma en el mensaje del sistema y el texto del usuario en un mensaje de usuario.\n",
    "\n",
    "La entrada de esta plantilla de mensaje es un diccionario. Podemos experimentar con esta plantilla de mensaje por sí sola para ver qué hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f62cad10-38b7-4765-9d18-6a3e486fee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd3e8f6-5365-41ab-b954-45ebbc99e7f2",
   "metadata": {},
   "source": [
    "Podemos ver que devuelve un ChatPromptValue que consta de dos mensajes. Si queremos acceder a los mensajes directamente, hacemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7319de5d-0480-47c8-8464-d1d6e4645030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc07da-16a9-4d2b-94c4-e45d43cff952",
   "metadata": {},
   "source": [
    "Finalmente, podemos invocar el modelo de chat en el mensaje formateado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf9c9cec-0d6a-4ecf-9de8-bb85aa7c3f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain Ollama)",
   "language": "python",
   "name": "langchain-ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
