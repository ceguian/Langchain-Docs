{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bfd5cf-2d35-4fd7-b218-a37d772c9cb1",
   "metadata": {},
   "source": [
    "# Construir un Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00a8e1-f6d3-442b-ae15-520af033014b",
   "metadata": {},
   "source": [
    "## Descripción general\n",
    "Repasaremos un ejemplo de cómo diseñar e implementar un chatbot con tecnología LLM. Este chatbot podrá mantener una conversación y recordar interacciones previas con un modelo de chat.\n",
    "\n",
    "Tenga en cuenta que este chatbot que construimos solo usará el modelo de lenguaje para mantener una conversación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8ba8e-61e7-407f-9906-700f0d28962d",
   "metadata": {},
   "source": [
    "## 📦 Instalación de dependencias necesarias\n",
    "\n",
    "Antes de ejecutar este notebook, asegúrate de instalar las siguientes bibliotecas:\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain_core langgraph typing-extensions transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f992f52-79b9-425e-a1d7-6b9d31760428",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de501e-2eba-4029-b910-ac96d1bf0712",
   "metadata": {},
   "source": [
    "## Inicio rápido\n",
    "Primero, aprendamos a usar un modelo de lenguaje por sí solo. LangChain admite muchos modelos de lenguaje diferentes que puedes usar indistintamente. En este notebook usaremos Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc780c2a-7e17-47b0-9c90-21145d2933f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = ChatOllama(\n",
    "    # Modelo descargado desde Ollama (puedes cambiar el nombre según el modelo disponible)\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434\"  # URL local donde corre el servidor de Ollama\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457925a-b682-488e-8038-aba1ac4e24fc",
   "metadata": {},
   "source": [
    "Primero, usemos el modelo directamente. Los ChatModels son instancias de los \"Runnables\" de LangChain, lo que significa que exponen una interfaz estándar para interactuar con ellos. Para llamar al modelo, podemos pasar una lista de mensajes al método .invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab77e94-a27e-46c5-b1a3-04d78e632946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Bob! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-10T05:29:30.2203278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3087216200, 'load_duration': 34353000, 'prompt_eval_count': 30, 'prompt_eval_duration': 122001600, 'eval_count': 27, 'eval_duration': 2930861600, 'model_name': 'llama3.2'}, id='run-0ddb2355-1786-4228-a874-2e5f3436cf05-0', usage_metadata={'input_tokens': 30, 'output_tokens': 27, 'total_tokens': 57})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the model\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec686f8-87aa-4dbc-90f2-524f1538e2bb",
   "metadata": {},
   "source": [
    "El modelo por sí solo no tiene concepto de estado. Por ejemplo, si se plantea una pregunta complementaria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99554ada-0516-4cfb-b32c-94c8b063cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your identity. I'm a large language model, I don't retain personal data or track individual users. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. Would you like to introduce yourself?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-10T05:32:54.5515372Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7975182800, 'load_duration': 45290200, 'prompt_eval_count': 30, 'prompt_eval_duration': 675378700, 'eval_count': 59, 'eval_duration': 7253896900, 'model_name': 'llama3.2'}, id='run-37a461cc-4d3a-472f-9d91-c123d5267c89-0', usage_metadata={'input_tokens': 30, 'output_tokens': 59, 'total_tokens': 89})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model without state\n",
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c9765-d6ed-47a4-88b1-9b90256c4075",
   "metadata": {},
   "source": [
    "Vemos que no toma en contexto la conversación anterior y no puede responder a la pregunta. Esto genera una experiencia de chatbot pésima.\n",
    "\n",
    "Para solucionar esto, necesitamos pasar todo el historial de conversaciones al modelo. Veamos qué sucede al hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd8af47-6979-4f6f-9d19-cb7b064bc1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Bob. That's the name we established at the beginning of our conversation.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-10T05:33:02.5549638Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5136282500, 'load_duration': 51295500, 'prompt_eval_count': 55, 'prompt_eval_duration': 2743480800, 'eval_count': 19, 'eval_duration': 2332604800, 'model_name': 'llama3.2'}, id='run-509088b9-4aa7-40e3-ae99-861a3fee76bf-0', usage_metadata={'input_tokens': 55, 'output_tokens': 19, 'total_tokens': 74})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Giving the context\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "213dfb06-883a-42f6-a35d-6a3bf909f6cb",
   "metadata": {},
   "source": [
    "¡Y ahora vemos que obtenemos una buena respuesta!\n",
    "\n",
    "Esta es la idea básica que sustenta la capacidad de un chatbot para interactuar conversacionalmente. Entonces, ¿cómo podemos implementarla mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281ac9a-4092-41e0-af0c-8fcbff3d9ab2",
   "metadata": {},
   "source": [
    "## Persistencia de mensajes\n",
    "\n",
    "LangGraph implementa una capa de persistencia integrada, lo que lo hace ideal para aplicaciones de chat que admiten múltiples turnos de conversación.\n",
    "\n",
    "Al integrar nuestro modelo de chat en una aplicación LangGraph minimalista, podemos persistir automáticamente el historial de mensajes, simplificando así el desarrollo de aplicaciones multiturno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7576c793-ddec-4680-98fc-492469eb6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac355811-e019-45f3-8713-e903b0764ae7",
   "metadata": {},
   "source": [
    "Ahora necesitamos crear una configuración que pasemos al ejecutable cada vez. Esta configuración contiene información que no forma parte de la entrada directa, pero que sigue siendo útil. En este caso, queremos incluir un thread_id. Debería verse así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1472aaec-4338-499e-a214-5a9bdfee9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6d8de-504f-4dc6-86ae-7384c97c318f",
   "metadata": {},
   "source": [
    "Esto nos permite gestionar múltiples hilos de conversación con una sola aplicación, un requisito común cuando la aplicación tiene varios usuarios.\n",
    "\n",
    "Podemos entonces invocar la aplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818c6b9e-4a5d-4ab3-88ea-5cb2eac4248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversación con el thread id: abc123\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# New query\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "# input uses the new query\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# invoke the workflow compile using the input and the thread\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "print(\n",
    "    f\"\\nConversación con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "\n",
    "# Printing the last message of the state\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8210fb2c-06c1-4a14-bcc4-8bff58fe0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. You told me that earlier when we started chatting.\n"
     ]
    }
   ],
   "source": [
    "# Following the thread\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9f81b-8666-4823-b6ec-44ffd2765fc6",
   "metadata": {},
   "source": [
    "¡Genial! Nuestro chatbot ahora recuerda información sobre nosotros. Si cambiamos la configuración para que haga referencia a un thread_id diferente, podemos ver que inicia la conversación desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d527ac-9e30-4056-bc1b-a2910f3d98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversación con el thread id: abc234\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. I'm a large language model, I don't have the ability to retain information about individual users or keep track of personal identities. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations.\n",
      "\n",
      "If you'd like to share your name with me, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "# Using a new thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "print(\n",
    "    f\"\\nConversación con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e85ea9-e909-40db-b44e-edb5d733a5d2",
   "metadata": {},
   "source": [
    "Sin embargo, siempre podemos volver a la conversación original (ya que la estamos conservando en una base de datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3090d82-5f51-454a-858d-4ff88d6822ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversación con el thread id: abc123\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned your name earlier as \"Bob\". However, I don't have any information about you beyond our conversation starting point. If you'd like to share more about yourself or change the subject, I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "print(\n",
    "    f\"\\nConversación con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd3edf-84dd-43a7-b37c-594317b5a0aa",
   "metadata": {},
   "source": [
    "¡Así es como podemos ayudar a un chatbot a mantener conversaciones con muchos usuarios!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc173401-8ccb-4749-a702-fa23be50b7a5",
   "metadata": {},
   "source": [
    "## Llamadas asíncronas\n",
    "Para obtener soporte asincrónico, actualice el nodo call_model para que sea una función asincrónica y use .ainvoke al invocar la aplicación:\n",
    "```Python\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710960d-ac0c-447b-a44a-f5010df2d793",
   "metadata": {},
   "source": [
    "Por ahora, solo hemos añadido una capa de persistencia simple alrededor del modelo. Podemos empezar a hacer el chatbot más complejo y personalizado añadiendo una plantilla de solicitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2f890-1473-4bb6-8e71-e762ddd6c08b",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Las plantillas de avisos (Prompt Templates) ayudan a convertir la información sin procesar del usuario a un formato compatible con el LLM. En este caso, la entrada sin procesar del usuario es simplemente un mensaje que pasamos al LLM. Ahora, compliquemos un poco más el proceso. Primero, añadiremos un mensaje del sistema con algunas instrucciones personalizadas (pero que seguirá aceptando mensajes como entrada). A continuación, añadiremos más información además de los mensajes.\n",
    "\n",
    "Para añadir un mensaje del sistema, crearemos una plantilla ChatPromptTemplate. Utilizaremos MessagesPlaceholder para pasar todos los mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9e8abd-c0d4-401d-b4d5-066936fec417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",  # sytem prompt\n",
    "        ),\n",
    "        # from state(MessageState) we use the messages to insert as the placeholder, the MessageState automatically will add any other message\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5992be3-3300-4b31-9939-3518ff3cdfcf",
   "metadata": {},
   "source": [
    "Ahora podemos actualizar nuestra aplicación para incorporar esta plantilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e1f190-2194-4684-877d-d0a757fb7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new workflow with the MessagesState schema\n",
    "builder = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# New function addgin the state\n",
    "def call_model_template(state: MessagesState):\n",
    "    # the prompt template will invoke the messages from the MessagesState schema\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    # the model will invoke the prompt with the full conversation\n",
    "    response = model.invoke(prompt)\n",
    "    # updating the messages from MessagesState with the new response\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Flow of the graph\n",
    "builder.add_edge(START, \"model_template\")\n",
    "# Node of the flow and the function called\n",
    "builder.add_node(\"model_template\", call_model_template)\n",
    "# Memory of the flow\n",
    "memory = MemorySaver()\n",
    "app = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d20319-4951-4a8f-b071-e91279db1c4b",
   "metadata": {},
   "source": [
    "Invocamos la aplicación de la misma manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8cbc72-391b-4511-ac53-724565f81970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversación con el thread id: abc345\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yer lookin' fer some scurvy-fightin' chat, eh? Well, matey Jim, welcome aboard! Ol' Blackbeak be here ta help ye navigate any waters o' knowledge or trouble that come yer way. What be bringin' ye to these fair shores today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "print(\n",
    "    f\"\\nConversación con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d171349-3c56-46b4-824c-4ce78e4627d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Arrr, me hearty Jim... *checks me trusty logbook* ...ye be known as... Jim! Aye, that be right! Ye don't need ta worry about forgettin' yer own name, matey. Now, what be ye lookin' fer knowledge about today?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78048db2-ef02-4281-977d-457b2290c763",
   "metadata": {},
   "source": [
    "¡Genial! Ahora compliquemos un poco más nuestra propuesta. Supongamos que la plantilla de propuesta ahora luce así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce60963b-1f5c-4b6c-a3a0-52ff7ad5e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complicate Prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df030fba-af7c-4175-b0c8-fabfbafec118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new state class with TypedDict schema: use messages as an Annotaded List with a\n",
    "# Sequence of BaseMessage using the method add_messages: Merges two lists of messages, updating existing messages by ID.\n",
    "# Use the variable language to be use it in the prompt template\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]    # List of messages\n",
    "    language: str                                               # lenguage variable\n",
    "\n",
    "\n",
    "# New workflow using the State class as schema\n",
    "flow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model_complicated(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "flow.add_edge(START, \"model_complicated\")\n",
    "flow.add_node(\"model_complicated\", call_model_complicated)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = flow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffab62f6-ed86-4197-b32a-92a32ba42ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversación con el thread id: abc456\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, Bob! (Hello, Bob!) ¿En qué puedo ayudarte hoy? (How can I help you today?)\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "# Defining the language\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(  # invoke the app with the two variables used in the schema and the thread\n",
    "    {\"messages\": input_messages,\n",
    "     \"language\": language},\n",
    "    config,\n",
    ")\n",
    "print(\n",
    "    f\"\\nConversación con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7451482-bb1c-468a-ba3a-0e2c6d59aefd",
   "metadata": {},
   "source": [
    "Tenga en cuenta que se conserva todo el estado, por lo que podemos omitir parámetros como el idioma si no deseamos realizar cambios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59742f2d-c66f-4cf4-8692-7c8efcc2e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. (Your name is Bob.) ¿Quieres saber algo más sobre ti mismo, Bob? (Do you want to know something more about yourself, Bob?)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa6647-9429-495f-b97d-7303a5b478ac",
   "metadata": {},
   "source": [
    "## Gestión del historial de conversaciones\n",
    "\n",
    "Un concepto importante al crear chatbots es cómo gestionar el historial de conversaciones. Si no se gestiona, la lista de mensajes crecerá sin límites y podría desbordar la ventana de contexto del LLM. Por lo tanto, es fundamental añadir un paso que limite el tamaño de los mensajes que se pasan.\n",
    "\n",
    "Es importante hacer esto ANTES de la plantilla de solicitud, pero DESPUÉS de cargar los mensajes anteriores del Historial de mensajes.\n",
    "\n",
    "Podemos lograrlo añadiendo un paso sencillo delante de la solicitud que modifique la clave de mensajes correctamente y, a continuación, envuelva esa nueva cadena en la clase Historial de mensajes.\n",
    "\n",
    "LangChain incluye varios ayudantes integrados para gestionar la lista de mensajes. En este caso, usaremos el ayudante trim_messages para reducir la cantidad de mensajes que enviamos al modelo. El ayudante trim_messages nos permite especificar cuántos tokens queremos conservar, junto con otros parámetros, como si queremos conservar siempre el mensaje del sistema y si se permiten mensajes parciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de7bb4bb-69ec-4795-959f-4c9e874ea75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.utils import count_tokens_approximately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b059ace7-6761-4587-b7eb-b2d9bdb2fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Invocando el trimmer\n",
      "\n",
      "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}), AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Managing Conversation History\n",
    "# Use trim_messages to reduce how many messages we're sending to the model.\n",
    "# use transformers\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=count_tokens_approximately,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# List of messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "print(\"\\nInvocando el trimmer\\n\")\n",
    "print(trimmer.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bfeb9-a4c4-4214-b3bf-9745761daf8a",
   "metadata": {},
   "source": [
    "To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0371fac-68bf-4423-8fc1-43f9be79962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newworkflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model_trimmed(state: State):\n",
    "    # Invoke the trimmer with the message of the state\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        # Passing the trimmed messages as messages in the prompt template\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "newworkflow.add_edge(START, \"model_trimmed\")\n",
    "newworkflow.add_node(\"model_trimmed\", call_model_trimmed)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = newworkflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bddfb-6238-44f1-a860-2fa090350f0a",
   "metadata": {},
   "source": [
    "Ahora, si intentamos preguntarle nuestro nombre al modelo, no lo sabrá porque hemos recortado esa parte del historial de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "896f9d50-9d9d-4aa3-9292-dca6a4dff02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have any information about your name. You didn't tell me, and I'm just a text-based AI assistant, I don't have the ability to store or recall personal data. Each time you interact with me, it's a new conversation! Would you like to tell me your name?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f688f5-32a6-4cf6-943a-8f5c50adca1e",
   "metadata": {},
   "source": [
    "Pero si le preguntamos por información que está dentro de los últimos mensajes, recuerda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb383b38-ab61-4ffe-8754-1cf88f077285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked the classic \"2 + 2\" problem!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38091c-5d25-4e22-9571-bb808a1e9ef2",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Ahora tenemos un chatbot funcionando. Sin embargo, una consideración clave para la experiencia de usuario (UX) en aplicaciones de chatbot es la transmisión. Los LLM a veces tardan en responder, por lo que, para mejorar la experiencia del usuario, la mayoría de las aplicaciones transmiten cada token a medida que se genera. Esto permite al usuario ver el progreso.\n",
    "\n",
    "¡Es realmente muy fácil!\n",
    "\n",
    "Por defecto, .stream en nuestra aplicación LangGraph transmite los pasos de la aplicación; en este caso, el paso de la respuesta del modelo. Configurar stream_mode=\"messages\" nos permite transmitir los tokens de salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aa4e0a4-8bbe-4777-907d-1c0de6483e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming\n",
      "\n",
      "Nice| to| meet| you|,| Todd|!| Here|'s| one|:\n",
      "\n",
      "|What| do| you| call| a| fake| nood|le|?\n",
      "\n",
      "|(wait| for| it|...)\n",
      "\n",
      "|An| imp|asta|!\n",
      "\n",
      "|Hope| that| made| you| laugh|,| Todd|!| Do| you| want| another| one|?||"
     ]
    }
   ],
   "source": [
    "print(\"\\nStreaming\\n\")\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain Ollama)",
   "language": "python",
   "name": "langchain-ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
