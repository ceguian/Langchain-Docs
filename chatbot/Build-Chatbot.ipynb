{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bfd5cf-2d35-4fd7-b218-a37d772c9cb1",
   "metadata": {},
   "source": [
    "# Construir un Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00a8e1-f6d3-442b-ae15-520af033014b",
   "metadata": {},
   "source": [
    "## Descripci칩n general\n",
    "Repasaremos un ejemplo de c칩mo dise침ar e implementar un chatbot con tecnolog칤a LLM. Este chatbot podr치 mantener una conversaci칩n y recordar interacciones previas con un modelo de chat.\n",
    "\n",
    "Tenga en cuenta que este chatbot que construimos solo usar치 el modelo de lenguaje para mantener una conversaci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8ba8e-61e7-407f-9906-700f0d28962d",
   "metadata": {},
   "source": [
    "## 游닍 Instalaci칩n de dependencias necesarias\n",
    "\n",
    "Antes de ejecutar este notebook, aseg칰rate de instalar las siguientes bibliotecas:\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain_core langgraph typing-extensions transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f992f52-79b9-425e-a1d7-6b9d31760428",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de501e-2eba-4029-b910-ac96d1bf0712",
   "metadata": {},
   "source": [
    "## Inicio r치pido\n",
    "Primero, aprendamos a usar un modelo de lenguaje por s칤 solo. LangChain admite muchos modelos de lenguaje diferentes que puedes usar indistintamente. En este notebook usaremos Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc780c2a-7e17-47b0-9c90-21145d2933f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = ChatOllama(\n",
    "    # Modelo descargado desde Ollama (puedes cambiar el nombre seg칰n el modelo disponible)\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434\"  # URL local donde corre el servidor de Ollama\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457925a-b682-488e-8038-aba1ac4e24fc",
   "metadata": {},
   "source": [
    "Primero, usemos el modelo directamente. Los ChatModels son instancias de los \"Runnables\" de LangChain, lo que significa que exponen una interfaz est치ndar para interactuar con ellos. Para llamar al modelo, podemos pasar una lista de mensajes al m칠todo .invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab77e94-a27e-46c5-b1a3-04d78e632946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Bob! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-10T05:29:30.2203278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3087216200, 'load_duration': 34353000, 'prompt_eval_count': 30, 'prompt_eval_duration': 122001600, 'eval_count': 27, 'eval_duration': 2930861600, 'model_name': 'llama3.2'}, id='run-0ddb2355-1786-4228-a874-2e5f3436cf05-0', usage_metadata={'input_tokens': 30, 'output_tokens': 27, 'total_tokens': 57})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the model\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec686f8-87aa-4dbc-90f2-524f1538e2bb",
   "metadata": {},
   "source": [
    "El modelo por s칤 solo no tiene concepto de estado. Por ejemplo, si se plantea una pregunta complementaria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99554ada-0516-4cfb-b32c-94c8b063cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your identity. I'm a large language model, I don't retain personal data or track individual users. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. Would you like to introduce yourself?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-10T05:32:54.5515372Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7975182800, 'load_duration': 45290200, 'prompt_eval_count': 30, 'prompt_eval_duration': 675378700, 'eval_count': 59, 'eval_duration': 7253896900, 'model_name': 'llama3.2'}, id='run-37a461cc-4d3a-472f-9d91-c123d5267c89-0', usage_metadata={'input_tokens': 30, 'output_tokens': 59, 'total_tokens': 89})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model without state\n",
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c9765-d6ed-47a4-88b1-9b90256c4075",
   "metadata": {},
   "source": [
    "Vemos que no toma en contexto la conversaci칩n anterior y no puede responder a la pregunta. Esto genera una experiencia de chatbot p칠sima.\n",
    "\n",
    "Para solucionar esto, necesitamos pasar todo el historial de conversaciones al modelo. Veamos qu칠 sucede al hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd8af47-6979-4f6f-9d19-cb7b064bc1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Bob. That's the name we established at the beginning of our conversation.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-10T05:33:02.5549638Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5136282500, 'load_duration': 51295500, 'prompt_eval_count': 55, 'prompt_eval_duration': 2743480800, 'eval_count': 19, 'eval_duration': 2332604800, 'model_name': 'llama3.2'}, id='run-509088b9-4aa7-40e3-ae99-861a3fee76bf-0', usage_metadata={'input_tokens': 55, 'output_tokens': 19, 'total_tokens': 74})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Giving the context\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "213dfb06-883a-42f6-a35d-6a3bf909f6cb",
   "metadata": {},
   "source": [
    "춰Y ahora vemos que obtenemos una buena respuesta!\n",
    "\n",
    "Esta es la idea b치sica que sustenta la capacidad de un chatbot para interactuar conversacionalmente. Entonces, 쯖칩mo podemos implementarla mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281ac9a-4092-41e0-af0c-8fcbff3d9ab2",
   "metadata": {},
   "source": [
    "## Persistencia de mensajes\n",
    "\n",
    "LangGraph implementa una capa de persistencia integrada, lo que lo hace ideal para aplicaciones de chat que admiten m칰ltiples turnos de conversaci칩n.\n",
    "\n",
    "Al integrar nuestro modelo de chat en una aplicaci칩n LangGraph minimalista, podemos persistir autom치ticamente el historial de mensajes, simplificando as칤 el desarrollo de aplicaciones multiturno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7576c793-ddec-4680-98fc-492469eb6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac355811-e019-45f3-8713-e903b0764ae7",
   "metadata": {},
   "source": [
    "Ahora necesitamos crear una configuraci칩n que pasemos al ejecutable cada vez. Esta configuraci칩n contiene informaci칩n que no forma parte de la entrada directa, pero que sigue siendo 칰til. En este caso, queremos incluir un thread_id. Deber칤a verse as칤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1472aaec-4338-499e-a214-5a9bdfee9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6d8de-504f-4dc6-86ae-7384c97c318f",
   "metadata": {},
   "source": [
    "Esto nos permite gestionar m칰ltiples hilos de conversaci칩n con una sola aplicaci칩n, un requisito com칰n cuando la aplicaci칩n tiene varios usuarios.\n",
    "\n",
    "Podemos entonces invocar la aplicaci칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818c6b9e-4a5d-4ab3-88ea-5cb2eac4248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversaci칩n con el thread id: abc123\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# New query\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "# input uses the new query\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# invoke the workflow compile using the input and the thread\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "print(\n",
    "    f\"\\nConversaci칩n con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "\n",
    "# Printing the last message of the state\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8210fb2c-06c1-4a14-bcc4-8bff58fe0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. You told me that earlier when we started chatting.\n"
     ]
    }
   ],
   "source": [
    "# Following the thread\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9f81b-8666-4823-b6ec-44ffd2765fc6",
   "metadata": {},
   "source": [
    "춰Genial! Nuestro chatbot ahora recuerda informaci칩n sobre nosotros. Si cambiamos la configuraci칩n para que haga referencia a un thread_id diferente, podemos ver que inicia la conversaci칩n desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d527ac-9e30-4056-bc1b-a2910f3d98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversaci칩n con el thread id: abc234\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. I'm a large language model, I don't have the ability to retain information about individual users or keep track of personal identities. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations.\n",
      "\n",
      "If you'd like to share your name with me, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "# Using a new thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "print(\n",
    "    f\"\\nConversaci칩n con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e85ea9-e909-40db-b44e-edb5d733a5d2",
   "metadata": {},
   "source": [
    "Sin embargo, siempre podemos volver a la conversaci칩n original (ya que la estamos conservando en una base de datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3090d82-5f51-454a-858d-4ff88d6822ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversaci칩n con el thread id: abc123\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned your name earlier as \"Bob\". However, I don't have any information about you beyond our conversation starting point. If you'd like to share more about yourself or change the subject, I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "print(\n",
    "    f\"\\nConversaci칩n con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd3edf-84dd-43a7-b37c-594317b5a0aa",
   "metadata": {},
   "source": [
    "춰As칤 es como podemos ayudar a un chatbot a mantener conversaciones con muchos usuarios!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc173401-8ccb-4749-a702-fa23be50b7a5",
   "metadata": {},
   "source": [
    "## Llamadas as칤ncronas\n",
    "Para obtener soporte asincr칩nico, actualice el nodo call_model para que sea una funci칩n asincr칩nica y use .ainvoke al invocar la aplicaci칩n:\n",
    "```Python\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710960d-ac0c-447b-a44a-f5010df2d793",
   "metadata": {},
   "source": [
    "Por ahora, solo hemos a침adido una capa de persistencia simple alrededor del modelo. Podemos empezar a hacer el chatbot m치s complejo y personalizado a침adiendo una plantilla de solicitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2f890-1473-4bb6-8e71-e762ddd6c08b",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Las plantillas de avisos (Prompt Templates) ayudan a convertir la informaci칩n sin procesar del usuario a un formato compatible con el LLM. En este caso, la entrada sin procesar del usuario es simplemente un mensaje que pasamos al LLM. Ahora, compliquemos un poco m치s el proceso. Primero, a침adiremos un mensaje del sistema con algunas instrucciones personalizadas (pero que seguir치 aceptando mensajes como entrada). A continuaci칩n, a침adiremos m치s informaci칩n adem치s de los mensajes.\n",
    "\n",
    "Para a침adir un mensaje del sistema, crearemos una plantilla ChatPromptTemplate. Utilizaremos MessagesPlaceholder para pasar todos los mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9e8abd-c0d4-401d-b4d5-066936fec417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",  # sytem prompt\n",
    "        ),\n",
    "        # from state(MessageState) we use the messages to insert as the placeholder, the MessageState automatically will add any other message\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5992be3-3300-4b31-9939-3518ff3cdfcf",
   "metadata": {},
   "source": [
    "Ahora podemos actualizar nuestra aplicaci칩n para incorporar esta plantilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e1f190-2194-4684-877d-d0a757fb7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new workflow with the MessagesState schema\n",
    "builder = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# New function addgin the state\n",
    "def call_model_template(state: MessagesState):\n",
    "    # the prompt template will invoke the messages from the MessagesState schema\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    # the model will invoke the prompt with the full conversation\n",
    "    response = model.invoke(prompt)\n",
    "    # updating the messages from MessagesState with the new response\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Flow of the graph\n",
    "builder.add_edge(START, \"model_template\")\n",
    "# Node of the flow and the function called\n",
    "builder.add_node(\"model_template\", call_model_template)\n",
    "# Memory of the flow\n",
    "memory = MemorySaver()\n",
    "app = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d20319-4951-4a8f-b071-e91279db1c4b",
   "metadata": {},
   "source": [
    "Invocamos la aplicaci칩n de la misma manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8cbc72-391b-4511-ac53-724565f81970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversaci칩n con el thread id: abc345\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yer lookin' fer some scurvy-fightin' chat, eh? Well, matey Jim, welcome aboard! Ol' Blackbeak be here ta help ye navigate any waters o' knowledge or trouble that come yer way. What be bringin' ye to these fair shores today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "print(\n",
    "    f\"\\nConversaci칩n con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d171349-3c56-46b4-824c-4ce78e4627d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Arrr, me hearty Jim... *checks me trusty logbook* ...ye be known as... Jim! Aye, that be right! Ye don't need ta worry about forgettin' yer own name, matey. Now, what be ye lookin' fer knowledge about today?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78048db2-ef02-4281-977d-457b2290c763",
   "metadata": {},
   "source": [
    "춰Genial! Ahora compliquemos un poco m치s nuestra propuesta. Supongamos que la plantilla de propuesta ahora luce as칤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce60963b-1f5c-4b6c-a3a0-52ff7ad5e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complicate Prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df030fba-af7c-4175-b0c8-fabfbafec118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new state class with TypedDict schema: use messages as an Annotaded List with a\n",
    "# Sequence of BaseMessage using the method add_messages: Merges two lists of messages, updating existing messages by ID.\n",
    "# Use the variable language to be use it in the prompt template\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]    # List of messages\n",
    "    language: str                                               # lenguage variable\n",
    "\n",
    "\n",
    "# New workflow using the State class as schema\n",
    "flow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model_complicated(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "flow.add_edge(START, \"model_complicated\")\n",
    "flow.add_node(\"model_complicated\", call_model_complicated)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = flow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffab62f6-ed86-4197-b32a-92a32ba42ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversaci칩n con el thread id: abc456\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, Bob! (Hello, Bob!) 쮼n qu칠 puedo ayudarte hoy? (How can I help you today?)\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "# Defining the language\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(  # invoke the app with the two variables used in the schema and the thread\n",
    "    {\"messages\": input_messages,\n",
    "     \"language\": language},\n",
    "    config,\n",
    ")\n",
    "print(\n",
    "    f\"\\nConversaci칩n con el thread id: {config['configurable']['thread_id']}\\n\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7451482-bb1c-468a-ba3a-0e2c6d59aefd",
   "metadata": {},
   "source": [
    "Tenga en cuenta que se conserva todo el estado, por lo que podemos omitir par치metros como el idioma si no deseamos realizar cambios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59742f2d-c66f-4cf4-8692-7c8efcc2e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. (Your name is Bob.) 쯈uieres saber algo m치s sobre ti mismo, Bob? (Do you want to know something more about yourself, Bob?)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa6647-9429-495f-b97d-7303a5b478ac",
   "metadata": {},
   "source": [
    "## Gesti칩n del historial de conversaciones\n",
    "\n",
    "Un concepto importante al crear chatbots es c칩mo gestionar el historial de conversaciones. Si no se gestiona, la lista de mensajes crecer치 sin l칤mites y podr칤a desbordar la ventana de contexto del LLM. Por lo tanto, es fundamental a침adir un paso que limite el tama침o de los mensajes que se pasan.\n",
    "\n",
    "Es importante hacer esto ANTES de la plantilla de solicitud, pero DESPU칄S de cargar los mensajes anteriores del Historial de mensajes.\n",
    "\n",
    "Podemos lograrlo a침adiendo un paso sencillo delante de la solicitud que modifique la clave de mensajes correctamente y, a continuaci칩n, envuelva esa nueva cadena en la clase Historial de mensajes.\n",
    "\n",
    "LangChain incluye varios ayudantes integrados para gestionar la lista de mensajes. En este caso, usaremos el ayudante trim_messages para reducir la cantidad de mensajes que enviamos al modelo. El ayudante trim_messages nos permite especificar cu치ntos tokens queremos conservar, junto con otros par치metros, como si queremos conservar siempre el mensaje del sistema y si se permiten mensajes parciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de7bb4bb-69ec-4795-959f-4c9e874ea75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.utils import count_tokens_approximately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b059ace7-6761-4587-b7eb-b2d9bdb2fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Invocando el trimmer\n",
      "\n",
      "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}), AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Managing Conversation History\n",
    "# Use trim_messages to reduce how many messages we're sending to the model.\n",
    "# use transformers\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=count_tokens_approximately,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# List of messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "print(\"\\nInvocando el trimmer\\n\")\n",
    "print(trimmer.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bfeb9-a4c4-4214-b3bf-9745761daf8a",
   "metadata": {},
   "source": [
    "To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0371fac-68bf-4423-8fc1-43f9be79962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newworkflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model_trimmed(state: State):\n",
    "    # Invoke the trimmer with the message of the state\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        # Passing the trimmed messages as messages in the prompt template\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "newworkflow.add_edge(START, \"model_trimmed\")\n",
    "newworkflow.add_node(\"model_trimmed\", call_model_trimmed)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = newworkflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bddfb-6238-44f1-a860-2fa090350f0a",
   "metadata": {},
   "source": [
    "Ahora, si intentamos preguntarle nuestro nombre al modelo, no lo sabr치 porque hemos recortado esa parte del historial de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "896f9d50-9d9d-4aa3-9292-dca6a4dff02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have any information about your name. You didn't tell me, and I'm just a text-based AI assistant, I don't have the ability to store or recall personal data. Each time you interact with me, it's a new conversation! Would you like to tell me your name?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f688f5-32a6-4cf6-943a-8f5c50adca1e",
   "metadata": {},
   "source": [
    "Pero si le preguntamos por informaci칩n que est치 dentro de los 칰ltimos mensajes, recuerda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb383b38-ab61-4ffe-8754-1cf88f077285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked the classic \"2 + 2\" problem!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38091c-5d25-4e22-9571-bb808a1e9ef2",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Ahora tenemos un chatbot funcionando. Sin embargo, una consideraci칩n clave para la experiencia de usuario (UX) en aplicaciones de chatbot es la transmisi칩n. Los LLM a veces tardan en responder, por lo que, para mejorar la experiencia del usuario, la mayor칤a de las aplicaciones transmiten cada token a medida que se genera. Esto permite al usuario ver el progreso.\n",
    "\n",
    "춰Es realmente muy f치cil!\n",
    "\n",
    "Por defecto, .stream en nuestra aplicaci칩n LangGraph transmite los pasos de la aplicaci칩n; en este caso, el paso de la respuesta del modelo. Configurar stream_mode=\"messages\" nos permite transmitir los tokens de salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aa4e0a4-8bbe-4777-907d-1c0de6483e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming\n",
      "\n",
      "Nice| to| meet| you|,| Todd|!| Here|'s| one|:\n",
      "\n",
      "|What| do| you| call| a| fake| nood|le|?\n",
      "\n",
      "|(wait| for| it|...)\n",
      "\n",
      "|An| imp|asta|!\n",
      "\n",
      "|Hope| that| made| you| laugh|,| Todd|!| Do| you| want| another| one|?||"
     ]
    }
   ],
   "source": [
    "print(\"\\nStreaming\\n\")\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain Ollama)",
   "language": "python",
   "name": "langchain-ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
